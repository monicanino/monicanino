{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSIA Demo-8_3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"L1oi7vxbpx1Y","colab_type":"text"},"source":["<div>\n","<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"b2ElPMh7px1b","colab_type":"text"},"source":["# Demo 8.3: Boosting\n","\n","INSTRUCTIONS:\n","\n","- Run the cells\n","- Observe and understand the results\n","- Answer the questions"]},{"cell_type":"markdown","metadata":{"id":"1OgzTRZzpx1d","colab_type":"text"},"source":["This is an excerpt from the [Ensemble Learning to Improve Machine Learning Results-How ensemble methods work: bagging, boosting and stacking](https://blog.statsbot.co/ensemble-learning-d1dcd548e936) by **Vadim Smolyakov**."]},{"cell_type":"markdown","metadata":{"id":"-NkM-HJepx1g","colab_type":"text"},"source":["## Boosting\n","**Boosting** refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) to weighted versions of the data, where more weight is given to examples that were mis-classified by earlier rounds. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods such as bagging is that base learners are trained in sequence on a weighted version of the data."]},{"cell_type":"code","metadata":{"id":"r17vtOS3px1i","colab_type":"code","colab":{}},"source":["## Import Libraries\n","\n","import itertools\n","import numpy as np\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import seaborn as sns\n","\n","from sklearn import datasets\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score, train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","from mlxtend.plotting import plot_learning_curves\n","from mlxtend.plotting import plot_decision_regions"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_bxP3ytipx1o","colab_type":"text"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"pySokxbkpx1q","colab_type":"code","colab":{}},"source":["## Loading the dataset\n","\n","iris = datasets.load_iris()\n","\n","# picking just the first two features\n","X = iris.data[:, 0:2]\n","# target\n","y = iris.target"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rv_ECRCBpx1u","colab_type":"code","colab":{}},"source":["## Check the data\n","\n","# About data\n","print(X.shape)\n","print(X[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWxT8i2Vpx1z","colab_type":"code","colab":{}},"source":["# About target\n","print(y.shape)\n","print(y[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyDzICRepx13","colab_type":"text"},"source":["## Modelling"]},{"cell_type":"code","metadata":{"id":"Ng8Sv0FOpx14","colab_type":"code","colab":{}},"source":["np.random.seed(0)\n","clf = DecisionTreeClassifier(criterion = 'entropy', max_depth = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OHxXrs3cpx17","colab_type":"text"},"source":["## Presenting results"]},{"cell_type":"code","metadata":{"id":"iwdaFAqopx18","colab_type":"code","colab":{}},"source":["num_est = [1, 2, 3, 10]\n","label = ['AdaBoost (n_est=%d)' % i for i in num_est]\n","\n","fig = plt.figure(figsize = (10, 8))\n","gs = gridspec.GridSpec(2, 2)\n","grid = itertools.product([0, 1], repeat = 2)\n","\n","for n_est, label, grd in zip(num_est, label, grid):     \n","    boosting = AdaBoostClassifier(base_estimator = clf, n_estimators = n_est)   \n","    boosting.fit(X, y)\n","    ax = plt.subplot(gs[grd[0], grd[1]])\n","    fig = plot_decision_regions(X = X, y = y, clf = boosting, legend = 2)\n","    plt.title(label)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qg0pVW4_px2A","colab_type":"text"},"source":["The AdaBoost algorithm is illustrated in the figure above. Each base learner consists of a decision tree with depth $1$, thus classifying the data based on a feature threshold that partitions the space into two regions separated by a linear decision surface that is parallel to one of the axes."]},{"cell_type":"code","metadata":{"id":"jj2BVwKhpx2B","colab_type":"code","colab":{}},"source":["# plot learning curves\n","np.random.seed(2534)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n","\n","boosting = AdaBoostClassifier(base_estimator = clf, n_estimators = 10)\n","        \n","plt.figure()\n","plot_learning_curves(X_train, y_train, X_test, y_test, boosting, print_model = False, style = 'ggplot')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAvjILILpx2E","colab_type":"code","colab":{}},"source":["# Ensemble Size\n","num_est = np.linspace(1, 100, 20, dtype = np.int8)\n","\n","np.random.seed(2534)\n","\n","bg_clf_cv_mean = []\n","bg_clf_cv_std = []\n","for n_est in num_est:\n","    ada_clf = AdaBoostClassifier(base_estimator = clf, n_estimators = n_est)\n","    scores = cross_val_score(ada_clf, X, y, cv = 3, scoring = 'accuracy')\n","    bg_clf_cv_mean.append(scores.mean())\n","    bg_clf_cv_std.append(scores.std())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TK1MB5ERpx2I","colab_type":"code","colab":{}},"source":["plt.figure()\n","(_, caps, _) = plt.errorbar(\n","    num_est,\n","    bg_clf_cv_mean,\n","    yerr = bg_clf_cv_std,\n","    c = 'blue',\n","    fmt = '-o',\n","    capsize = 5)\n","\n","for cap in caps:\n","    cap.set_markeredgewidth(1)\n","\n","plt.title('AdaBoost Ensemble')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Ensemble Size')\n","plt.savefig('c23.png')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWfX0VVZpx2K","colab_type":"text"},"source":["The figure above shows how the test accuracy improves with the size of the ensemble.\n","\n","**Gradient Tree Boosting** is a generalization of boosting to arbitrary differentiable loss functions. It can be used for both regression and classification problems."]},{"cell_type":"markdown","metadata":{"id":"IS5Tc4z9FoYy","colab_type":"text"},"source":[">"]},{"cell_type":"markdown","metadata":{"id":"mxI2We9OFpfs","colab_type":"text"},"source":[">"]},{"cell_type":"markdown","metadata":{"id":"81DoNxN1FqGN","colab_type":"text"},"source":[">"]},{"cell_type":"markdown","metadata":{"id":"RERADKgNFq9T","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > Â© 2019 Institute of Data\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]}]}